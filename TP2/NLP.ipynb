{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports\n",
    "Ici on peut retrouver les importations nécéssaires pour le projet\n",
    "Marche à suivre pour sklearn: https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\WickkaWizz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\WickkaWizz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\WickkaWizz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# get punkt and perceptron average\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature Extraction\n",
    "Dans cette section nous allons extraire les features du texte en utilisant nltk pour générer les stop-words que nous allons enlever du texte, en faisant le stemming des données avec Porter, en faisant le POS tagging et pour terminer nous allons aussi établir une grammaire pour pouvoir voir la structure du texte."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Porter\n",
    "Ici nous avons le stemmer Porter et nous faisons quelques tests pour vérifier qu'il fonctionne"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemmer\n",
      "cat\n",
      "troubl\n",
      "troubl\n",
      "troubl\n"
     ]
    }
   ],
   "source": [
    "# https://www.datacamp.com/tutorial/stemming-lemmatization-python\n",
    "porter = PorterStemmer()\n",
    "print(\"Porter Stemmer\")\n",
    "print(porter.stem(\"cats\"))\n",
    "print(porter.stem(\"trouble\"))\n",
    "print(porter.stem(\"troubling\"))\n",
    "print(porter.stem(\"troubled\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Extraction des propriétés (Feature Extraction)\n",
    "Ceci est la fonction qui nous permet d'extraire les propriétés pour chaque phrase (feature)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def extract_features(tagged_sentence, index):\n",
    "    token, tag = tagged_sentence[index]\n",
    "    prev_token = \"\"\n",
    "    if index > 0:\n",
    "        prev_token, prev_tag = tagged_sentence[index - 1]\n",
    "    is_number = False\n",
    "    try:\n",
    "        if float(token):\n",
    "            is_number = True\n",
    "    except:\n",
    "        pass\n",
    "    features_dict = {\"token\": token\n",
    "        , \"lower_cased_token\": token.lower()\n",
    "        , \"prev_token\": prev_token\n",
    "        , \"suffix1\": token[-1]\n",
    "        , \"suffix2\": token[-2:]\n",
    "        , \"suffix3\": token[-3:]\n",
    "        , \"is_capitalized\": token.upper() == token\n",
    "        , \"is_number\": is_number}\n",
    "    return features_dict\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "# https://www.guru99.com/pos-tagging-chunking-nltk.html\n",
    "# https://www.datacamp.com/tutorial/stemming-lemmatization-python\n",
    "corpus = []\n",
    "f = open(\"data/interest-original.txt\", \"r\")\n",
    "for x in f:\n",
    "    corpus.append(x)\n",
    "# We remove all the $$ from the list to make it cleaner\n",
    "sentence_corpus = [i for i in corpus if i != \"$$\\n\"]\n",
    "#print(sentence_corpus[1])\n",
    "# We tokenize our corpus into words, for each sentence\n",
    "corpus_tokens = []\n",
    "for i in sentence_corpus:\n",
    "    corpus_tokens.append(nltk.word_tokenize(i))\n",
    "#print(corpus_tokens)\n",
    "# We remove all the stop-words before stemming the text\n",
    "# We do a double for loop because we want to keep the structure of sentences and words intact\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "corpus_stop_words = []\n",
    "for i in corpus_tokens:\n",
    "    sw = []\n",
    "    for e in i:\n",
    "       if e.casefold() not in stop_words:\n",
    "           sw.append(e)\n",
    "    corpus_stop_words.append(sw)\n",
    "#print(corpus_stop_words)\n",
    "# We make the stemming of all the tokens\n",
    "# We do a double for loop because we want to keep the structure of sentences and words intact\n",
    "corpus_stemmed = []\n",
    "for i in corpus_stop_words:\n",
    "    stemming = []\n",
    "    for e in i:\n",
    "        stemming.append(porter.stem(e))\n",
    "    corpus_stemmed.append(stemming)\n",
    "# We do this list for the vectorizers, because they need a list of sentences\n",
    "sent_corpus_stemmed = []\n",
    "for i in corpus_stemmed:\n",
    "    sent_corpus_stemmed.append(\" \".join(i))\n",
    "#print(corpus_stemmed)\n",
    "# We make the POS tagging\n",
    "# corpus_tag = []\n",
    "# for i in corpus_stemmed:\n",
    "#     corpus_tag.append(nltk.pos_tag(i))\n",
    "corpus_tag = nltk.pos_tag_sents(corpus_stemmed)\n",
    "#print(corpus_tag)\n",
    "# We apply the grammar on the text, so we can get the structure. We get trees for each sentence\n",
    "# TODO verify if this grammar is enough for our text\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = []\n",
    "for i in corpus_tag:\n",
    "    result.append(cp.parse(i))\n",
    "#print(result)\n",
    "result[0].draw()    # It will draw the pattern graphically which can be seen in Noun Phrase chunking"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ici on ne fait que tester si le corpus est bien taggé et que la fonction d'extraction fonctionne bien"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "{'token': 'money-market',\n 'lower_cased_token': 'money-market',\n 'prev_token': 'yield',\n 'suffix1': 't',\n 'suffix2': 'et',\n 'suffix3': 'ket',\n 'is_capitalized': False,\n 'is_number': False}"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_features(corpus_tag[0], 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Datasets\n",
    "Ici nous faisons les jeux de données pour nos algorithmes d'apprentissage. Nous allons prendre 80% des données pour l'ensemble d'apprentissage et nous allons garder 20% pour les données de test."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape : (38990, 18033)\n",
      "X_test shape : (9773, 18033)\n",
      "Length of Y_train : 38990\n",
      "Length of Y_test : 9773\n"
     ]
    }
   ],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html\n",
    "# Training dataset for X (80%)\n",
    "X_features = []\n",
    "floor = (len(corpus_tag)*0.8).__floor__()\n",
    "ceiling = (len(corpus_tag)*0.8).__ceil__()\n",
    "for sentence in corpus_tag[0:floor]:\n",
    "    for k in range(len(sentence)):\n",
    "        X_features.append(extract_features(sentence, k))\n",
    "\n",
    "# We are using this vectorizer because our feature extractor returns a dictionary\n",
    "vectoriser = DictVectorizer(sparse=False)\n",
    "X_train = vectoriser.fit_transform(X_features)\n",
    "print(f\"X_train shape : {X_train.shape}\")\n",
    "\n",
    "# Testing dataset for X (20%)\n",
    "X_features = []\n",
    "for sentence in corpus_tag[ceiling:]:\n",
    "    for k in range(len(sentence)):\n",
    "        X_features.append(extract_features(sentence, k))\n",
    "\n",
    "# Important: We only want to transform the data, since we need the same number of features\n",
    "X_test = vectoriser.transform(X_features)\n",
    "print(f\"X_test shape : {X_test.shape}\")\n",
    "\n",
    "# Training labels for X_train (80%)\n",
    "Y_train = []\n",
    "for sentence in corpus_tag[0:floor]:\n",
    "    for k in range(len(sentence)):\n",
    "        Y_train.append(sentence[k][1])\n",
    "print(f\"Length of Y_train : {len(Y_train)}\")\n",
    "\n",
    "# Training labels for X_test (20%)\n",
    "Y_test = []\n",
    "for sentence in corpus_tag[ceiling:]:\n",
    "    for k in range(len(sentence)):\n",
    "        Y_test.append(sentence[k][1])\n",
    "print(f\"Length of Y_test : {len(Y_test)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Naive Bayes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8000613936355264\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes Train\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, Y_train)\n",
    "# Naive Bayes predictions\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_true=Y_test, y_pred=predictions))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Arbre de Décision"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "dt = DecisionTreeClassifier()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Foret Aléatoire"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "rf = RandomForestClassifier()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SVM"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "svm = SVC()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Multi layer perceptron (MLP)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "mlp = MLPClassifier()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP longer/JJR maturities/NNS)\n",
      "  are/VBP\n",
      "  thought/VBN\n",
      "  to/TO\n",
      "  indicate/VB\n",
      "  (NP declining/VBG interest_6/NN rates/NNS)\n",
      "  because/IN\n",
      "  (NP they/PP)\n",
      "  permit/VBP\n",
      "  (NP portfolio/NN managers/NNS)\n",
      "  to/TO\n",
      "  retain/VB\n",
      "  relatively/RB\n",
      "  (NP higher/JJR rates/NNS)\n",
      "  for/IN\n",
      "  (NP a/DT longer/JJR period/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# https://www.nltk.org/howto/chunk.html\n",
    "# Le texte a déjà été chunk et pos tag\n",
    "from nltk import tagstr2tree\n",
    "\n",
    "gold_chunked_text = tagstr2tree(sentence_corpus[1])\n",
    "print(gold_chunked_text)\n",
    "gold_chunked_text.draw()\n",
    "#[nltk.tag.str2tuple(t) for t in gold_chunked_text]\n",
    "# print(tagged_token)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Count Vectorizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yield money-market mutual fund continu slide , amid sign portfolio manag expect declin interest_6 rate .\n",
      "00\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [2368, 5259]",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 12\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28mprint\u001B[39m(sent_corpus_stemmed[\u001B[38;5;241m0\u001B[39m])\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28mprint\u001B[39m(vectorizer\u001B[38;5;241m.\u001B[39mget_feature_names_out()[\u001B[38;5;241m0\u001B[39m])\n\u001B[1;32m---> 12\u001B[0m X_train, X_test, y_train, y_test \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_test_split\u001B[49m\u001B[43m(\u001B[49m\u001B[43msent_corpus_stemmed\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvectorizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_feature_names_out\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m42\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     13\u001B[0m X_count \u001B[38;5;241m=\u001B[39m vectorizer\u001B[38;5;241m.\u001B[39mfit_transform(X_train)\n\u001B[0;32m     15\u001B[0m tfvectorizer \u001B[38;5;241m=\u001B[39m TfidfTransformer()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2445\u001B[0m, in \u001B[0;36mtrain_test_split\u001B[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001B[0m\n\u001B[0;32m   2442\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n_arrays \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m   2443\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAt least one array required as input\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m-> 2445\u001B[0m arrays \u001B[38;5;241m=\u001B[39m \u001B[43mindexable\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43marrays\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2447\u001B[0m n_samples \u001B[38;5;241m=\u001B[39m _num_samples(arrays[\u001B[38;5;241m0\u001B[39m])\n\u001B[0;32m   2448\u001B[0m n_train, n_test \u001B[38;5;241m=\u001B[39m _validate_shuffle_split(\n\u001B[0;32m   2449\u001B[0m     n_samples, test_size, train_size, default_test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.25\u001B[39m\n\u001B[0;32m   2450\u001B[0m )\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:433\u001B[0m, in \u001B[0;36mindexable\u001B[1;34m(*iterables)\u001B[0m\n\u001B[0;32m    414\u001B[0m \u001B[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001B[39;00m\n\u001B[0;32m    415\u001B[0m \n\u001B[0;32m    416\u001B[0m \u001B[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    429\u001B[0m \u001B[38;5;124;03m    sparse matrix, or dataframe) or `None`.\u001B[39;00m\n\u001B[0;32m    430\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    432\u001B[0m result \u001B[38;5;241m=\u001B[39m [_make_indexable(X) \u001B[38;5;28;01mfor\u001B[39;00m X \u001B[38;5;129;01min\u001B[39;00m iterables]\n\u001B[1;32m--> 433\u001B[0m \u001B[43mcheck_consistent_length\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mresult\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    434\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:387\u001B[0m, in \u001B[0;36mcheck_consistent_length\u001B[1;34m(*arrays)\u001B[0m\n\u001B[0;32m    385\u001B[0m uniques \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39munique(lengths)\n\u001B[0;32m    386\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(uniques) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m--> 387\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    388\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound input variables with inconsistent numbers of samples: \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    389\u001B[0m         \u001B[38;5;241m%\u001B[39m [\u001B[38;5;28mint\u001B[39m(l) \u001B[38;5;28;01mfor\u001B[39;00m l \u001B[38;5;129;01min\u001B[39;00m lengths]\n\u001B[0;32m    390\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: Found input variables with inconsistent numbers of samples: [2368, 5259]"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# https://www.nltk.org/book/ch06.html\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "# https://medium.com/@eiki1212/natural-language-processing-naive-bayes-classification-in-python-e934365cf40c\n",
    "# https://nlp1000.wordpress.com/2016/12/19/pos-tagging-scikit-learn/\n",
    "# ngram_range=(1,2) I might use this later, but I'm not too sure I need it right now\n",
    "vectorizer = CountVectorizer(input=\"content\", analyzer='word')\n",
    "vectorizer.fit_transform(sent_corpus_stemmed)\n",
    "print(sent_corpus_stemmed[0])\n",
    "print(vectorizer.get_feature_names_out()[0])\n",
    "X_train, X_test, y_train, y_test = train_test_split(sent_corpus_stemmed, vectorizer.get_feature_names_out(), test_size=0.2, random_state=42)\n",
    "X_count = vectorizer.fit_transform(X_train)\n",
    "\n",
    "tfvectorizer = TfidfTransformer()\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_count)\n",
    "\n",
    "# Create model(naive bayes) and training.\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_true=y_test, y_pred=predictions))\n",
    "\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X_count.toarray())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TF-IDF Vectorizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00' '000' '01' ... 'zinc' 'zoet' 'zurich']\n",
      "(2368, 5259)\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer\n",
    "Tfvectorizer = TfidfVectorizer(input=\"content\", analyzer='word')\n",
    "X_tfidf = vectorizer.fit_transform(sent_corpus_stemmed)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X_tfidf.shape)\n",
    "print(X_tfidf.toarray())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Stop words"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# https://realpython.com/nltk-nlp-python/#:~:text=Natural%20language%20processing%20(NLP)%20is,and%20contains%20human%2Dreadable%20text\n",
    "# https://stackoverflow.com/questions/52712254/how-to-eliminate-stop-words-only-using-scikit-learn\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "filtered_list = [word for word in words_in_quote if word.casefold() not in stop_words]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}