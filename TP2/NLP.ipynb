{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports\n",
    "Ici on peut retrouver les importations nécéssaires pour le projet\n",
    "Marche à suivre pour sklearn: https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\WickkaWizz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\WickkaWizz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\WickkaWizz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# get punkt and perceptron average\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature Extraction\n",
    "Dans cette section nous allons extraire les features du texte en utilisant nltk pour générer les stop-words que nous allons enlever du texte, en faisant le stemming des données avec Porter, en faisant le POS tagging et pour terminer nous allons aussi établir une grammaire pour pouvoir voir la structure du texte."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Porter\n",
    "Ici nous avons le stemmer Porter et nous faisons quelques tests pour vérifier qu'il fonctionne"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemmer\n",
      "cat\n",
      "troubl\n",
      "troubl\n",
      "troubl\n"
     ]
    }
   ],
   "source": [
    "# https://www.datacamp.com/tutorial/stemming-lemmatization-python\n",
    "porter = PorterStemmer()\n",
    "print(\"Porter Stemmer\")\n",
    "print(porter.stem(\"cats\"))\n",
    "print(porter.stem(\"trouble\"))\n",
    "print(porter.stem(\"troubling\"))\n",
    "print(porter.stem(\"troubled\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Extraction des propriétés (Feature Extraction)\n",
    "Ceci est la fonction qui nous permet d'extraire les propriétés pour chaque phrase (feature)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def extract_features(tagged_sentence, index):\n",
    "    token, tag = tagged_sentence[index]\n",
    "    prev_token1 = \"\"\n",
    "    prev_token2 = \"\"\n",
    "    next_token1 = \"\"\n",
    "    next_token2 = \"\"\n",
    "    prev_tag1 = \"\"\n",
    "    prev_tag2 = \"\"\n",
    "    next_tag1 = \"\"\n",
    "    next_tag2 = \"\"\n",
    "    if index > 0:\n",
    "        prev_token1, prev_tag1 = tagged_sentence[index - 1]\n",
    "    if index > 1:\n",
    "        prev_token2, prev_tag2 = tagged_sentence[index - 2]\n",
    "    if index < len(tagged_sentence)-1:\n",
    "        next_token1, next_tag1 = tagged_sentence[index + 1]\n",
    "    if index < len(tagged_sentence)-2:\n",
    "        next_token2, next_tag2 = tagged_sentence[index + 2]\n",
    "    is_number = False\n",
    "    try:\n",
    "        if float(token):\n",
    "            is_number = True\n",
    "    except:\n",
    "        pass\n",
    "    # Different lists for different window size around the words\n",
    "    # features_dict = {\"token\": token\n",
    "    #     , \"lower_cased_token\": token.lower()\n",
    "    #     , \"prev_token-1\": prev_token1\n",
    "    #     , \"prev_tag-1\": prev_tag1\n",
    "    #     , \"prev_token-2\": prev_token2\n",
    "    #     , \"prev_tag-2\": prev_tag2\n",
    "    #     , \"next_token1\": next_token1\n",
    "    #     , \"next_tag-1\": next_tag1\n",
    "    #     , \"next_token2\": next_token2\n",
    "    #     , \"next_tag-2\": next_tag2\n",
    "    #     , \"suffix1\": token[-1]\n",
    "    #     , \"suffix2\": token[-2:]\n",
    "    #     , \"suffix3\": token[-3:]\n",
    "    #     , \"is_capitalized\": token.upper() == token\n",
    "    #     , \"is_number\": is_number}\n",
    "    features_dict = {\"token\": token\n",
    "        , \"lower_cased_token\": token.lower()\n",
    "        , \"prev_token-1\": prev_token1\n",
    "        , \"prev_tag-1\": prev_tag1\n",
    "        , \"next_token1\": next_token1\n",
    "        , \"next_tag-1\": next_tag1\n",
    "        , \"suffix1\": token[-1]\n",
    "        , \"suffix2\": token[-2:]\n",
    "        , \"suffix3\": token[-3:]\n",
    "        , \"is_capitalized\": token.upper() == token\n",
    "        , \"is_number\": is_number}\n",
    "    # features_dict = {\"token\": token\n",
    "    #     , \"lower_cased_token\": token.lower()\n",
    "    #     , \"prev_token-1\": prev_token1\n",
    "    #     , \"prev_tag-1\": prev_tag1\n",
    "    #     , \"suffix1\": token[-1]\n",
    "    #     , \"suffix2\": token[-2:]\n",
    "    #     , \"suffix3\": token[-3:]\n",
    "    #     , \"is_capitalized\": token.upper() == token\n",
    "    #     , \"is_number\": is_number}\n",
    "    return features_dict\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "# https://www.guru99.com/pos-tagging-chunking-nltk.html\n",
    "# https://www.datacamp.com/tutorial/stemming-lemmatization-python\n",
    "corpus = []\n",
    "f = open(\"data/interest-original.txt\", \"r\")\n",
    "for x in f:\n",
    "    corpus.append(x)\n",
    "# We remove all the $$ from the list to make it cleaner\n",
    "sentence_corpus = [i for i in corpus if i != \"$$\\n\"]\n",
    "#print(sentence_corpus[1])\n",
    "# We tokenize our corpus into words, for each sentence\n",
    "corpus_tokens = []\n",
    "for i in sentence_corpus:\n",
    "    corpus_tokens.append(nltk.word_tokenize(i))\n",
    "#print(corpus_tokens)\n",
    "# We remove all the stop-words before stemming the text\n",
    "# We do a double for loop because we want to keep the structure of sentences and words intact\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "corpus_stop_words = []\n",
    "for i in corpus_tokens:\n",
    "    sw = []\n",
    "    for e in i:\n",
    "       if e.casefold() not in stop_words:\n",
    "           sw.append(e)\n",
    "    corpus_stop_words.append(sw)\n",
    "#print(corpus_stop_words)\n",
    "# We make the stemming of all the tokens\n",
    "# We do a double for loop because we want to keep the structure of sentences and words intact\n",
    "corpus_stemmed = []\n",
    "for i in corpus_stop_words:\n",
    "    stemming = []\n",
    "    for e in i:\n",
    "        stemming.append(porter.stem(e))\n",
    "    corpus_stemmed.append(stemming)\n",
    "# We do this list for the vectorizers, because they need a list of sentences\n",
    "sent_corpus_stemmed = []\n",
    "for i in corpus_stemmed:\n",
    "    sent_corpus_stemmed.append(\" \".join(i))\n",
    "#print(corpus_stemmed)\n",
    "# Switch between them whenever you want to test the stemmed vs un-stemmed corpus\n",
    "corpus_tag = nltk.pos_tag_sents(corpus_stemmed)\n",
    "#corpus_tag = nltk.pos_tag_sents(corpus_stop_words)\n",
    "#print(corpus_tag)\n",
    "# We apply the grammar on the text, so we can get the structure. We get trees for each sentence\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = []\n",
    "for i in corpus_tag:\n",
    "    result.append(cp.parse(i))\n",
    "#print(result)\n",
    "result[0].draw()    # It will draw the pattern graphically which can be seen in Noun Phrase chunking"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ici on ne fait que tester si le corpus est bien taggé et que la fonction d'extraction fonctionne bien"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "{'token': 'money-market',\n 'lower_cased_token': 'money-market',\n 'prev_token-1': 'yield',\n 'prev_tag-1': 'NN',\n 'next_token1': 'mutual',\n 'next_tag-1': 'JJ',\n 'suffix1': 't',\n 'suffix2': 'et',\n 'suffix3': 'ket',\n 'is_capitalized': False,\n 'is_number': False}"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_features(corpus_tag[0], 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Datasets\n",
    "Ici nous faisons les jeux de données pour nos algorithmes d'apprentissage. Nous allons prendre 80% des données pour l'ensemble d'apprentissage et nous allons garder 20% pour les données de test.\n",
    "Nous avons créé une méthode qui prend les données dans un ordre spécifique pour pouvoir tester plus facilement la précision des algorithmes. Cependant, nous sommes conscients que ce n'est pas recommandé pour réellement tester si les algorithmes sont précis, car normalement nous devons faire des jeux de données aléatoires pour s'assurer que les algorithmes n'ont pas simplement eu un coup de chance. La deuxième méthode qui est implémentée par SKlearn, nous permet de mettre une seed et de controler les jeux de données avec cette seed. Utiliser celle qui vous semble adéquat, par contre soyez avisé que celle de SKlearn coûte beaucoup plus chère en mémoire vive que celle que nous avons implémenté."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html\n",
    "# Training dataset for X (80%)\n",
    "X_features = []\n",
    "floor = (len(corpus_tag)*0.8).__floor__()\n",
    "ceiling = (len(corpus_tag)*0.8).__ceil__()\n",
    "for sentence in corpus_tag[0:floor]:\n",
    "    for k in range(len(sentence)):\n",
    "        X_features.append(extract_features(sentence, k))\n",
    "\n",
    "# We are using this vectorizer because our feature extractor returns a dictionary\n",
    "vectoriser = DictVectorizer(sparse=False)\n",
    "X_train = vectoriser.fit_transform(X_features)\n",
    "print(f\"X_train shape : {X_train.shape}\")\n",
    "\n",
    "# Testing dataset for X (20%)\n",
    "X_features = []\n",
    "for sentence in corpus_tag[ceiling:]:\n",
    "    for k in range(len(sentence)):\n",
    "        X_features.append(extract_features(sentence, k))\n",
    "\n",
    "# Important: We only want to transform the data, since we need the same number of features\n",
    "X_test = vectoriser.transform(X_features)\n",
    "print(f\"X_test shape : {X_test.shape}\")\n",
    "\n",
    "# Training labels for X_train (80%)\n",
    "Y_train = []\n",
    "for sentence in corpus_tag[0:floor]:\n",
    "    for k in range(len(sentence)):\n",
    "        Y_train.append(sentence[k][1])\n",
    "print(f\"Length of Y_train : {len(Y_train)}\")\n",
    "\n",
    "# Training labels for X_test (20%)\n",
    "Y_test = []\n",
    "for sentence in corpus_tag[ceiling:]:\n",
    "    for k in range(len(sentence)):\n",
    "        Y_test.append(sentence[k][1])\n",
    "print(f\"Length of Y_test : {len(Y_test)}\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape : (39027, 26210)\n",
      "X_test shape : (9757, 26210)\n",
      "Length of Y_train : 39027\n",
      "Length of Y_test : 9757\n"
     ]
    }
   ],
   "source": [
    "X_features = []\n",
    "for sentence in corpus_tag:\n",
    "    for k in range(len(sentence)):\n",
    "        X_features.append(extract_features(sentence, k))\n",
    "\n",
    "vectoriser = DictVectorizer(sparse=False)\n",
    "X = vectoriser.fit_transform(X_features)\n",
    "\n",
    "y = []\n",
    "for sentence in corpus_tag:\n",
    "    for k in range(len(sentence)):\n",
    "        y.append(sentence[k][1])\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "print(f\"X_train shape : {X_train.shape}\")\n",
    "print(f\"X_test shape : {X_test.shape}\")\n",
    "print(f\"Length of Y_train : {len(Y_train)}\")\n",
    "print(f\"Length of Y_test : {len(Y_test)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Naive Bayes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8344778108025007\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes Train\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, Y_train)\n",
    "# Naive Bayes predictions\n",
    "predictions = clf.predict(X_test)\n",
    "# Corpus Stemmed\n",
    "# 0.8000613936355264 (previous token)\n",
    "# 0.8061904273854669 (prev1, prev2, next1, next2)\n",
    "# 0.8202316285743568 (prev1, next1)\n",
    "\n",
    "# Corpus un-stemmed\n",
    "# 0.8344778108025007 (previous token)\n",
    "# 0.8446243722455673 (prev1, next1)\n",
    "# 0.82781592702675 (prev1, prev2, next1, next2)\n",
    "print(accuracy_score(y_true=Y_test, y_pred=predictions))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Arbre de Décision"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WickkaWizz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_split.py:684: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "params = {\"criterion\": [\"gini\"]\n",
    "        , \"splitter\": [\"best\"]\n",
    "        , \"max_depth\": [5]\n",
    "        , \"min_samples_split\": [2, 5, 10]\n",
    "          }\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "cv = GridSearchCV(estimator=dt, param_grid = params)\n",
    "cv.fit(X_train, Y_train)\n",
    "\n",
    "# Best parameter set\n",
    "print('Best parameters found:\\n', cv.best_params_)\n",
    "\n",
    "# All results\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "# Decision tree classifier with gini for the cost function\n",
    "# Trop de noeuds et de dimensions, je dois limiter la profondeur pour eviter le sur-apprentissage\n",
    "dt = DecisionTreeClassifier(criterion=\"gini\", splitter=\"best\", max_depth=20, min_samples_split=5)\n",
    "dt.fit(X_train, Y_train)\n",
    "predictions = dt.predict(X_test)\n",
    "print(f\"Accuracy of Decision Tree with gini: {accuracy_score(y_true=Y_test, y_pred=predictions)}\")\n",
    "# Accuracy of Decision Tree with gini: 0.6826893512350107\n",
    "\n",
    "dt = DecisionTreeClassifier(criterion=\"entropy\", splitter=\"best\", max_depth=20, min_samples_split=5)\n",
    "dt.fit(X_train, Y_train)\n",
    "predictions = dt.predict(X_test)\n",
    "print(f\"Accuracy of Decision Tree with entropy: {accuracy_score(y_true=Y_test, y_pred=predictions)}\")\n",
    "# Accuracy of Decision Tree with entropy: 0.7336271394895972"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Foret Aléatoire"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Random Forest Tree with gini: 0.625909603361689\n",
      "Accuracy of Random Forest Tree with entropy: 0.6430255201393871\n"
     ]
    }
   ],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "rf = RandomForestClassifier(n_estimators=50, criterion=\"gini\", max_depth=20, min_samples_split=5)\n",
    "rf.fit(X_train, Y_train)\n",
    "predictions = rf.predict(X_test)\n",
    "print(f\"Accuracy of Random Forest Tree with gini: {accuracy_score(y_true=Y_test, y_pred=predictions)}\")\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=50, criterion=\"entropy\", max_depth=20, min_samples_split=5)\n",
    "rf.fit(X_train, Y_train)\n",
    "predictions = rf.predict(X_test)\n",
    "print(f\"Accuracy of Random Forest Tree with entropy: {accuracy_score(y_true=Y_test, y_pred=predictions)}\")\n",
    "# Accuracy of Random Forest Tree with gini: 0.625909603361689\n",
    "# Accuracy of Random Forest Tree with entropy: 0.6430255201393871"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SVM"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WickkaWizz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=5).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Support Vector Machine with linear kernel: 0.343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WickkaWizz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=5).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Support Vector Machine with polynomial kernel: 0.587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WickkaWizz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=5).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Support Vector Machine with sigmoid kernel: 0.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WickkaWizz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Support Vector Machine with linear kernel and reduction of dimensionality: 0.4888797786204776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WickkaWizz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Support Vector Machine with polynomial kernel and reduction of dimensionality: 0.3679409654606949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WickkaWizz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Support Vector Machine with sigmoid kernel and reduction of dimensionality: 0.3519524443988931\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "# https://stats.stackexchange.com/questions/65094/why-scaling-is-important-for-the-linear-svm-classification\n",
    "# Les données ne sont pas linéairement séparables, donc on doit imposer une limite pour les itérations pour éviter que les SVM ne se termine jamais\n",
    "svm = SVC(C=1, kernel='linear', max_iter=5)\n",
    "svm.fit(X_train[0:1000, :], Y_train[0:1000])\n",
    "predictions = svm.predict(X_test[0:1000, :])\n",
    "print(f\"Accuracy of Support Vector Machine with linear kernel: {accuracy_score(y_true=Y_test[0:1000], y_pred=predictions)}\")\n",
    "# Accuracy of Support Vector Machine with linear kernel: 0.302 (iteration = 1)\n",
    "# Accuracy of Support Vector Machine with linear kernel: 0.343 (iteration = 5)\n",
    "\n",
    "svm = SVC(C=1, kernel='poly', max_iter=5)\n",
    "svm.fit(X_train[0:1000, :], Y_train[0:1000])\n",
    "predictions = svm.predict(X_test[0:1000, :])\n",
    "print(f\"Accuracy of Support Vector Machine with polynomial kernel: {accuracy_score(y_true=Y_test[0:1000], y_pred=predictions)}\")\n",
    "# Accuracy of Support Vector Machine with polynomial kernel: 0.346 (iteration = 1)\n",
    "# Accuracy of Support Vector Machine with polynomial kernel: 0.587 (iteration = 5)\n",
    "\n",
    "svm = SVC(C=1, kernel='sigmoid', max_iter=5)\n",
    "svm.fit(X_train[0:1000, :], Y_train[0:1000])\n",
    "predictions = svm.predict(X_test[0:1000, :])\n",
    "print(f\"Accuracy of Support Vector Machine with sigmoid kernel: {accuracy_score(y_true=Y_test[0:1000], y_pred=predictions)}\")\n",
    "# Accuracy of Support Vector Machine with sigmoid kernel: 0.458 (iteration = 1)\n",
    "# Accuracy of Support Vector Machine with sigmoid kernel: 0.57 (iteration = 5)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_principal = pca.fit_transform(X_train)\n",
    "svm = SVC(C=1, kernel='linear', max_iter=1000)\n",
    "svm.fit(X_principal, Y_train)\n",
    "predictions = svm.predict(pca.fit_transform(X_test))\n",
    "print(f\"Accuracy of Support Vector Machine with linear kernel and reduction of dimensionality: {accuracy_score(y_true=Y_test, y_pred=predictions)}\")\n",
    "# Accuracy of Support Vector Machine with linear kernel and reduction of dimensionality: 0.48396023367838475 (iteration = 1000)\n",
    "\n",
    "svm = SVC(C=1, kernel='poly', max_iter=1000)\n",
    "svm.fit(X_principal, Y_train)\n",
    "predictions = svm.predict(pca.fit_transform(X_test))\n",
    "print(f\"Accuracy of Support Vector Machine with polynomial kernel and reduction of dimensionality: {accuracy_score(y_true=Y_test, y_pred=predictions)}\")\n",
    "# Accuracy of Support Vector Machine with polynomial kernel and reduction of dimensionality: 0.5309008916675207 (iteration = 1000)\n",
    "\n",
    "\n",
    "svm = SVC(C=1, kernel='sigmoid', max_iter=1000)\n",
    "svm.fit(X_principal, Y_train)\n",
    "predictions = svm.predict(pca.fit_transform(X_test))\n",
    "print(f\"Accuracy of Support Vector Machine with sigmoid kernel and reduction of dimensionality: {accuracy_score(y_true=Y_test, y_pred=predictions)}\")\n",
    "# Accuracy of Support Vector Machine with sigmoid kernel and reduction of dimensionality: 0.37265552936353386 (iteration = 1000)\n",
    "\n",
    "# svm = SVC(C=1, kernel='poly', max_iter=100)\n",
    "# svm.fit(X_train, Y_train)\n",
    "# predictions = svm.predict(X_test)\n",
    "# print(f\"Accuracy of Support Vector Machine with polynomial kernel: {accuracy_score(y_true=Y_test, y_pred=predictions)}\")\n",
    "#\n",
    "# svm = SVC(C=1, kernel='sigmoid', max_iter=100)\n",
    "# svm.fit(X_train, Y_train)\n",
    "# predictions = svm.predict(X_test)\n",
    "# print(f\"Accuracy of Support Vector Machine with sigmoid kernel: {accuracy_score(y_true=Y_test, y_pred=predictions)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Multi layer perceptron (MLP)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WickkaWizz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_split.py:684: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:\n",
      " {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'max_iter': 500, 'solver': 'adam'}\n",
      "0.555 (+/-0.002) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'max_iter': 100, 'solver': 'sgd'}\n",
      "0.686 (+/-0.008) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'max_iter': 100, 'solver': 'adam'}\n",
      "0.605 (+/-0.011) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'sgd'}\n",
      "0.681 (+/-0.010) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'adam'}\n",
      "0.668 (+/-0.044) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'max_iter': 500, 'solver': 'sgd'}\n",
      "0.699 (+/-0.005) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'max_iter': 500, 'solver': 'adam'}\n",
      "0.555 (+/-0.002) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'max_iter': 100, 'solver': 'sgd'}\n",
      "0.688 (+/-0.025) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'max_iter': 100, 'solver': 'adam'}\n",
      "0.600 (+/-0.009) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'sgd'}\n",
      "0.688 (+/-0.012) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'adam'}\n",
      "0.672 (+/-0.003) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'sgd'}\n",
      "0.695 (+/-0.017) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'adam'}\n",
      "0.555 (+/-0.002) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'max_iter': 100, 'solver': 'sgd'}\n",
      "0.692 (+/-0.008) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'max_iter': 100, 'solver': 'adam'}\n",
      "0.595 (+/-0.029) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'sgd'}\n",
      "0.700 (+/-0.005) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'adam'}\n",
      "0.676 (+/-0.014) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'max_iter': 500, 'solver': 'sgd'}\n",
      "0.687 (+/-0.020) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'max_iter': 500, 'solver': 'adam'}\n",
      "0.555 (+/-0.002) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'max_iter': 100, 'solver': 'sgd'}\n",
      "0.689 (+/-0.017) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'max_iter': 100, 'solver': 'adam'}\n",
      "0.608 (+/-0.009) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'sgd'}\n",
      "0.694 (+/-0.014) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'adam'}\n",
      "0.679 (+/-0.004) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'sgd'}\n",
      "0.693 (+/-0.016) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'adam'}\n",
      "0.521 (+/-0.049) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'max_iter': 100, 'solver': 'sgd'}\n",
      "0.701 (+/-0.015) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'max_iter': 100, 'solver': 'adam'}\n",
      "0.563 (+/-0.002) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'sgd'}\n",
      "0.699 (+/-0.015) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'adam'}\n",
      "0.643 (+/-0.012) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'max_iter': 500, 'solver': 'sgd'}\n",
      "0.696 (+/-0.008) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'max_iter': 500, 'solver': 'adam'}\n",
      "0.504 (+/-0.001) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'max_iter': 100, 'solver': 'sgd'}\n",
      "0.700 (+/-0.013) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'max_iter': 100, 'solver': 'adam'}\n",
      "0.564 (+/-0.004) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'sgd'}\n",
      "0.695 (+/-0.007) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'adam'}\n",
      "0.643 (+/-0.005) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'sgd'}\n",
      "0.694 (+/-0.004) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'adam'}\n",
      "0.552 (+/-0.010) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'max_iter': 100, 'solver': 'sgd'}\n",
      "0.693 (+/-0.013) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'max_iter': 100, 'solver': 'adam'}\n",
      "0.596 (+/-0.033) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'sgd'}\n",
      "0.705 (+/-0.016) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'adam'}\n",
      "0.672 (+/-0.024) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'max_iter': 500, 'solver': 'sgd'}\n",
      "0.698 (+/-0.011) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'max_iter': 500, 'solver': 'adam'}\n",
      "0.557 (+/-0.007) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'max_iter': 100, 'solver': 'sgd'}\n",
      "0.704 (+/-0.012) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'max_iter': 100, 'solver': 'adam'}\n",
      "0.602 (+/-0.007) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'sgd'}\n",
      "0.692 (+/-0.019) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'adam'}\n",
      "0.680 (+/-0.019) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'sgd'}\n",
      "0.710 (+/-0.011) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'adam'}\n",
      "0.554 (+/-0.004) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'max_iter': 100, 'solver': 'sgd'}\n",
      "0.691 (+/-0.017) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'max_iter': 100, 'solver': 'adam'}\n",
      "0.607 (+/-0.017) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'sgd'}\n",
      "0.700 (+/-0.026) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'adam'}\n",
      "0.681 (+/-0.016) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'max_iter': 500, 'solver': 'sgd'}\n",
      "0.706 (+/-0.009) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'max_iter': 500, 'solver': 'adam'}\n",
      "0.555 (+/-0.002) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'max_iter': 100, 'solver': 'sgd'}\n",
      "0.701 (+/-0.013) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'max_iter': 100, 'solver': 'adam'}\n",
      "0.604 (+/-0.004) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'sgd'}\n",
      "0.701 (+/-0.013) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'adam'}\n",
      "0.674 (+/-0.019) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'sgd'}\n",
      "0.703 (+/-0.013) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'adam'}\n",
      "0.504 (+/-0.001) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'max_iter': 100, 'solver': 'sgd'}\n",
      "0.712 (+/-0.001) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'max_iter': 100, 'solver': 'adam'}\n",
      "0.563 (+/-0.002) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'sgd'}\n",
      "0.712 (+/-0.008) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'adam'}\n",
      "0.642 (+/-0.010) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'max_iter': 500, 'solver': 'sgd'}\n",
      "0.715 (+/-0.021) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'max_iter': 500, 'solver': 'adam'}\n",
      "0.516 (+/-0.018) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'max_iter': 100, 'solver': 'sgd'}\n",
      "0.712 (+/-0.008) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'max_iter': 100, 'solver': 'adam'}\n",
      "0.563 (+/-0.002) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'sgd'}\n",
      "0.710 (+/-0.015) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'adam'}\n",
      "0.644 (+/-0.018) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'sgd'}\n",
      "0.714 (+/-0.022) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'adam'}\n",
      "0.538 (+/-0.047) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'max_iter': 100, 'solver': 'sgd'}\n",
      "0.693 (+/-0.005) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'max_iter': 100, 'solver': 'adam'}\n",
      "0.593 (+/-0.031) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'sgd'}\n",
      "0.704 (+/-0.020) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'adam'}\n",
      "0.676 (+/-0.031) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'max_iter': 500, 'solver': 'sgd'}\n",
      "0.701 (+/-0.004) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'max_iter': 500, 'solver': 'adam'}\n",
      "0.555 (+/-0.002) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'max_iter': 100, 'solver': 'sgd'}\n",
      "0.702 (+/-0.028) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'max_iter': 100, 'solver': 'adam'}\n",
      "0.603 (+/-0.010) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'sgd'}\n",
      "0.696 (+/-0.019) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'adam'}\n",
      "0.664 (+/-0.031) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'sgd'}\n",
      "0.705 (+/-0.013) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'adam'}\n",
      "0.555 (+/-0.002) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'max_iter': 100, 'solver': 'sgd'}\n",
      "0.695 (+/-0.011) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'max_iter': 100, 'solver': 'adam'}\n",
      "0.596 (+/-0.051) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'sgd'}\n",
      "0.701 (+/-0.010) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'adam'}\n",
      "0.677 (+/-0.040) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'max_iter': 500, 'solver': 'sgd'}\n",
      "0.708 (+/-0.012) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'max_iter': 500, 'solver': 'adam'}\n",
      "0.560 (+/-0.009) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'max_iter': 100, 'solver': 'sgd'}\n",
      "0.697 (+/-0.011) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'max_iter': 100, 'solver': 'adam'}\n",
      "0.602 (+/-0.016) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'sgd'}\n",
      "0.701 (+/-0.029) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'adam'}\n",
      "0.679 (+/-0.027) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'sgd'}\n",
      "0.709 (+/-0.036) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'adam'}\n",
      "0.521 (+/-0.049) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'max_iter': 100, 'solver': 'sgd'}\n",
      "0.719 (+/-0.012) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'max_iter': 100, 'solver': 'adam'}\n",
      "0.563 (+/-0.002) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'sgd'}\n",
      "0.716 (+/-0.015) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'adam'}\n",
      "0.640 (+/-0.008) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'max_iter': 500, 'solver': 'sgd'}\n",
      "0.719 (+/-0.017) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'max_iter': 500, 'solver': 'adam'}\n",
      "0.516 (+/-0.035) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'max_iter': 100, 'solver': 'sgd'}\n",
      "0.715 (+/-0.009) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'max_iter': 100, 'solver': 'adam'}\n",
      "0.563 (+/-0.002) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'sgd'}\n",
      "0.719 (+/-0.015) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'adam'}\n",
      "0.638 (+/-0.016) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'sgd'}\n",
      "0.718 (+/-0.021) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'adam'}\n",
      "0.431 (+/-0.004) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'max_iter': 100, 'solver': 'sgd'}\n",
      "0.686 (+/-0.026) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'max_iter': 100, 'solver': 'adam'}\n",
      "0.471 (+/-0.059) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'sgd'}\n",
      "0.687 (+/-0.017) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'adam'}\n",
      "0.617 (+/-0.022) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'max_iter': 500, 'solver': 'sgd'}\n",
      "0.693 (+/-0.012) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'max_iter': 500, 'solver': 'adam'}\n",
      "0.431 (+/-0.004) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'max_iter': 100, 'solver': 'sgd'}\n",
      "0.692 (+/-0.019) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'max_iter': 100, 'solver': 'adam'}\n",
      "0.455 (+/-0.070) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'sgd'}\n",
      "0.684 (+/-0.015) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'adam'}\n",
      "0.624 (+/-0.012) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'sgd'}\n",
      "0.674 (+/-0.024) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'adam'}\n",
      "0.431 (+/-0.004) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'max_iter': 100, 'solver': 'sgd'}\n",
      "0.689 (+/-0.017) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'max_iter': 100, 'solver': 'adam'}\n",
      "0.480 (+/-0.067) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'sgd'}\n",
      "0.684 (+/-0.020) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'adam'}\n",
      "0.617 (+/-0.011) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'max_iter': 500, 'solver': 'sgd'}\n",
      "0.670 (+/-0.015) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'max_iter': 500, 'solver': 'adam'}\n",
      "0.431 (+/-0.004) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'max_iter': 100, 'solver': 'sgd'}\n",
      "0.682 (+/-0.021) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'max_iter': 100, 'solver': 'adam'}\n",
      "0.431 (+/-0.004) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'sgd'}\n",
      "0.685 (+/-0.010) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'adam'}\n",
      "0.603 (+/-0.034) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'sgd'}\n",
      "0.681 (+/-0.047) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'adam'}\n",
      "0.451 (+/-0.059) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'max_iter': 100, 'solver': 'sgd'}\n",
      "0.695 (+/-0.015) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'max_iter': 100, 'solver': 'adam'}\n",
      "0.557 (+/-0.007) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'sgd'}\n",
      "0.688 (+/-0.021) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'adam'}\n",
      "0.621 (+/-0.021) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'max_iter': 500, 'solver': 'sgd'}\n",
      "0.685 (+/-0.027) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'max_iter': 500, 'solver': 'adam'}\n",
      "0.431 (+/-0.004) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'max_iter': 100, 'solver': 'sgd'}\n",
      "0.694 (+/-0.017) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'max_iter': 100, 'solver': 'adam'}\n",
      "0.555 (+/-0.002) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'sgd'}\n",
      "0.694 (+/-0.029) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'adam'}\n",
      "0.618 (+/-0.011) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'sgd'}\n",
      "0.684 (+/-0.032) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'adam'}\n",
      "0.431 (+/-0.004) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'max_iter': 100, 'solver': 'sgd'}\n",
      "0.679 (+/-0.017) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'max_iter': 100, 'solver': 'adam'}\n",
      "0.458 (+/-0.067) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'sgd'}\n",
      "0.696 (+/-0.023) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'adam'}\n",
      "0.613 (+/-0.034) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'max_iter': 500, 'solver': 'sgd'}\n",
      "0.694 (+/-0.020) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'max_iter': 500, 'solver': 'adam'}\n",
      "0.431 (+/-0.004) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'max_iter': 100, 'solver': 'sgd'}\n",
      "0.692 (+/-0.015) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'max_iter': 100, 'solver': 'adam'}\n",
      "0.504 (+/-0.001) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'sgd'}\n",
      "0.694 (+/-0.008) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'adam'}\n",
      "0.614 (+/-0.015) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'sgd'}\n",
      "0.694 (+/-0.009) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'adam'}\n",
      "0.431 (+/-0.004) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'max_iter': 100, 'solver': 'sgd'}\n",
      "0.689 (+/-0.016) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'max_iter': 100, 'solver': 'adam'}\n",
      "0.431 (+/-0.004) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'sgd'}\n",
      "0.696 (+/-0.025) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'adam'}\n",
      "0.609 (+/-0.033) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'max_iter': 500, 'solver': 'sgd'}\n",
      "0.692 (+/-0.025) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'max_iter': 500, 'solver': 'adam'}\n",
      "0.431 (+/-0.004) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'max_iter': 100, 'solver': 'sgd'}\n",
      "0.679 (+/-0.030) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'max_iter': 100, 'solver': 'adam'}\n",
      "0.521 (+/-0.047) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'sgd'}\n",
      "0.686 (+/-0.020) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'adam'}\n",
      "0.622 (+/-0.013) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'sgd'}\n",
      "0.688 (+/-0.014) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'adam'}\n",
      "0.431 (+/-0.004) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'max_iter': 100, 'solver': 'sgd'}\n",
      "0.703 (+/-0.015) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'max_iter': 100, 'solver': 'adam'}\n",
      "0.555 (+/-0.002) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'sgd'}\n",
      "0.709 (+/-0.014) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'adam'}\n",
      "0.616 (+/-0.012) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'max_iter': 500, 'solver': 'sgd'}\n",
      "0.715 (+/-0.008) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'max_iter': 500, 'solver': 'adam'}\n",
      "0.431 (+/-0.004) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'max_iter': 100, 'solver': 'sgd'}\n",
      "0.702 (+/-0.023) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'max_iter': 100, 'solver': 'adam'}\n",
      "0.555 (+/-0.002) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'sgd'}\n",
      "0.706 (+/-0.010) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'adam'}\n",
      "0.618 (+/-0.024) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'sgd'}\n",
      "0.715 (+/-0.013) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'adam'}\n",
      "0.431 (+/-0.004) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'max_iter': 100, 'solver': 'sgd'}\n",
      "0.690 (+/-0.014) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'max_iter': 100, 'solver': 'adam'}\n",
      "0.472 (+/-0.118) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'sgd'}\n",
      "0.700 (+/-0.016) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'adam'}\n",
      "0.616 (+/-0.027) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'max_iter': 500, 'solver': 'sgd'}\n",
      "0.691 (+/-0.029) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'max_iter': 500, 'solver': 'adam'}\n",
      "0.431 (+/-0.004) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'max_iter': 100, 'solver': 'sgd'}\n",
      "0.696 (+/-0.015) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'max_iter': 100, 'solver': 'adam'}\n",
      "0.479 (+/-0.072) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'sgd'}\n",
      "0.695 (+/-0.011) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'adam'}\n",
      "0.610 (+/-0.030) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'sgd'}\n",
      "0.694 (+/-0.006) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'adam'}\n",
      "0.431 (+/-0.004) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'max_iter': 100, 'solver': 'sgd'}\n",
      "0.697 (+/-0.018) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'max_iter': 100, 'solver': 'adam'}\n",
      "0.479 (+/-0.072) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'sgd'}\n",
      "0.701 (+/-0.017) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'adam'}\n",
      "0.614 (+/-0.030) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'max_iter': 500, 'solver': 'sgd'}\n",
      "0.698 (+/-0.013) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'max_iter': 500, 'solver': 'adam'}\n",
      "0.431 (+/-0.004) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'max_iter': 100, 'solver': 'sgd'}\n",
      "0.697 (+/-0.045) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'max_iter': 100, 'solver': 'adam'}\n",
      "0.479 (+/-0.072) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'sgd'}\n",
      "0.705 (+/-0.024) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'adam'}\n",
      "0.598 (+/-0.038) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'sgd'}\n",
      "0.697 (+/-0.024) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'adam'}\n",
      "0.431 (+/-0.004) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'max_iter': 100, 'solver': 'sgd'}\n",
      "0.709 (+/-0.009) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'max_iter': 100, 'solver': 'adam'}\n",
      "0.555 (+/-0.002) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'sgd'}\n",
      "0.716 (+/-0.012) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'adam'}\n",
      "0.617 (+/-0.013) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'max_iter': 500, 'solver': 'sgd'}\n",
      "0.717 (+/-0.012) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'max_iter': 500, 'solver': 'adam'}\n",
      "0.431 (+/-0.004) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'max_iter': 100, 'solver': 'sgd'}\n",
      "0.710 (+/-0.019) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'max_iter': 100, 'solver': 'adam'}\n",
      "0.555 (+/-0.002) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'sgd'}\n",
      "0.709 (+/-0.010) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'adam'}\n",
      "0.610 (+/-0.014) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'sgd'}\n",
      "0.717 (+/-0.020) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'max_iter': 500, 'solver': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(50, 50, 50), (50, 100, 50), (100,)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.05, 0.1],\n",
    "    'learning_rate': ['constant', 'adaptive'],\n",
    "    'max_iter': [100, 200, 500]\n",
    "}\n",
    "\n",
    "mlp = MLPClassifier()\n",
    "clf = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=3)\n",
    "clf.fit(X_train[0:1000, :], Y_train[0:1000])\n",
    "\n",
    "# Best parameter set\n",
    "print('Best parameters found:\\n', clf.best_params_)\n",
    "\n",
    "# All results\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,), activation='tanh', alpha=0.1, learning_rate=\"constant\", max_iter = 500, solver = 'adam')\n",
    "mlp.fit(X_train, Y_train)\n",
    "predictions = mlp.predict(X_test)\n",
    "print(f\"Accuracy of Multi Level Perceptron with activation tanh : {accuracy_score(y_true=Y_test, y_pred=predictions)}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# https://www.nltk.org/howto/chunk.html\n",
    "# Le texte a déjà été chunk et pos tag\n",
    "from nltk import tagstr2tree\n",
    "\n",
    "gold_chunked_text = tagstr2tree(sentence_corpus[1])\n",
    "print(gold_chunked_text)\n",
    "gold_chunked_text.draw()\n",
    "#[nltk.tag.str2tuple(t) for t in gold_chunked_text]\n",
    "# print(tagged_token)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Count Vectorizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# https://www.nltk.org/book/ch06.html\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "# https://medium.com/@eiki1212/natural-language-processing-naive-bayes-classification-in-python-e934365cf40c\n",
    "# https://nlp1000.wordpress.com/2016/12/19/pos-tagging-scikit-learn/\n",
    "# ngram_range=(1,2) I might use this later, but I'm not too sure I need it right now\n",
    "vectorizer = CountVectorizer(input=\"content\", analyzer='word')\n",
    "vectorizer.fit_transform(sent_corpus_stemmed)\n",
    "print(sent_corpus_stemmed[0])\n",
    "print(vectorizer.get_feature_names_out()[0])\n",
    "X_train, X_test, y_train, y_test = train_test_split(sent_corpus_stemmed, vectorizer.get_feature_names_out(), test_size=0.2, random_state=42)\n",
    "X_count = vectorizer.fit_transform(X_train)\n",
    "\n",
    "tfvectorizer = TfidfTransformer()\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_count)\n",
    "\n",
    "# Create model(naive bayes) and training.\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_true=y_test, y_pred=predictions))\n",
    "\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X_count.toarray())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TF-IDF Vectorizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer\n",
    "Tfvectorizer = TfidfVectorizer(input=\"content\", analyzer='word')\n",
    "X_tfidf = vectorizer.fit_transform(sent_corpus_stemmed)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X_tfidf.shape)\n",
    "print(X_tfidf.toarray())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Stop words"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# https://realpython.com/nltk-nlp-python/#:~:text=Natural%20language%20processing%20(NLP)%20is,and%20contains%20human%2Dreadable%20text\n",
    "# https://stackoverflow.com/questions/52712254/how-to-eliminate-stop-words-only-using-scikit-learn\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "filtered_list = [word for word in words_in_quote if word.casefold() not in stop_words]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}